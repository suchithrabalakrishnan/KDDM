{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the pandas package, then use the \"read_csv\" function to read\n",
    "# the labeled training data\n",
    "import pandas as pd       \n",
    "train = pd.read_csv(\"smsspamcollection.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5453, 2)\n",
      "['message' 'class']\n",
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "5453\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(train.columns.values)\n",
    "print(train[\"message\"][0])\n",
    "num_reviews = train[\"message\"].size\n",
    "print(num_reviews)\n",
    "\n",
    "example=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><body><p>U dun say so early hor... U c already then say...</p></body></html>\n"
     ]
    }
   ],
   "source": [
    "# Import BeautifulSoup into your workspace\n",
    "from bs4 import BeautifulSoup             \n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "# Initialize the BeautifulSoup object on a single movie review\n",
    "    example.append (BeautifulSoup(train[\"message\"][i],\"lxml\")) \n",
    "\n",
    "\n",
    "# Print the raw review and then the output of get_text(), for \n",
    "# comparison\n",
    "#print (train[\"message\"][0])\n",
    "#print (example1.get_text())\n",
    "print(example[3])\n",
    "letters_only=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# Use regular expressions to do a find-and-replace\n",
    "for i in range(0,num_reviews):\n",
    "    letters_only.append (re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      example[i].get_text() ))  # The text to search\n",
    "for i in range(0,num_reviews):\n",
    "    cl=train[\"class\"][i]\n",
    "    letters_only[i]+=' '+cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your account has been refilled successfully by INR   DECIMAL    Your KeralaCircle prepaid account balance is Rs   DECIMAL    Your Transaction ID is KR  H    ham\n"
     ]
    }
   ],
   "source": [
    "print(letters_only[200])\n",
    "lower_case=[]\n",
    "words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxxmobilemovieclub', 'to', 'use', 'your', 'credit', 'click', 'the', 'wap', 'link', 'in', 'the', 'next', 'txt', 'message', 'or', 'click', 'here', 'http', 'wap', 'xxxmobilemovieclub', 'com', 'n', 'qjkgighjjgcbl', 'spam']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,num_reviews):\n",
    "    lower_case.append (letters_only[i].lower())        # Convert to lower case\n",
    "for i in range(0,num_reviews):\n",
    "    words.append (lower_case[i].split())               # Split into words\n",
    "\n",
    "    #print (letters_only[i])\n",
    "print(words[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "#print (stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from \"words\"\n",
    "for i in range(0,num_reviews):\n",
    "    woo=words[i]\n",
    "    woo = [w for w in woo if not w in stopwords.words(\"english\")]\n",
    "    words[i]=woo\n",
    "   \n",
    "    \n",
    "    #words[i]=woo\n",
    "#print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat', 'ham'], ['ok', 'lar', 'joking', 'wif', 'u', 'oni', 'ham'], ['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'c', 'apply', 'spam'], ['u', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say', 'ham'], ['nah', 'think', 'goes', 'usf', 'lives', 'around', 'though', 'ham'], ['freemsg', 'hey', 'darling', 'week', 'word', 'back', 'like', 'fun', 'still', 'tb', 'ok', 'xxx', 'std', 'chgs', 'send', 'ae', 'rcv', 'spam'], ['even', 'brother', 'like', 'speak', 'treat', 'like', 'aids', 'patent', 'ham'], ['per', 'request', 'melle', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'set', 'callertune', 'callers', 'press', 'e', 'copy', 'friends', 'callertune', 'ham'], ['winner', 'valued', 'network', 'customer', 'selected', 'receivea', 'ae', 'prize', 'reward', 'claim', 'call', 'claim', 'code', 'kl', 'valid', 'hours', 'spam'], ['mobile', 'months', 'u', 'r', 'entitled', 'update', 'latest', 'colour', 'mobiles', 'camera', 'free', 'call', 'mobile', 'update', 'co', 'free', 'spam']]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(words[0:10])\n",
    "print(type(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-2362406a01e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# into feature vectors. The input to fit_transform should be a list of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtrain_data_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Numpy arrays are easy to work with, so convert the result to an\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\suchi\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 839\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\suchi\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\suchi\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 241\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\suchi\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "print (\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(words)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "#print (vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "       print(\"wordcount found\")\n",
    "# print (count, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divides a set on a specific column. Can handle numeric or nominal values\n",
    "def divideset(rows,column,value):\n",
    "   #print(\"Entered\")\n",
    "   # Make a function that tells us if a row is in the first group (true) or the second group (false)\n",
    "   split_function=None\n",
    "   if isinstance(value,int) or isinstance(value,float): # check if the value is a number i.e int or float\n",
    "      split_function=lambda row:row[column]>=value\n",
    "   else:\n",
    "      split_function=lambda row:row[column]==value\n",
    "   \n",
    "   # Divide the rows into two sets and return them\n",
    "   set1=[row for row in rows if split_function(row)]\n",
    "   set2=[row for row in rows if not split_function(row)]\n",
    "   return (set1,set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ['free',\n",
       "  'entry',\n",
       "  'wkly',\n",
       "  'comp',\n",
       "  'win',\n",
       "  'fa',\n",
       "  'cup',\n",
       "  'final',\n",
       "  'tkts',\n",
       "  'st',\n",
       "  'may',\n",
       "  'text',\n",
       "  'fa',\n",
       "  'receive',\n",
       "  'entry',\n",
       "  'question',\n",
       "  'std',\n",
       "  'txt',\n",
       "  'rate',\n",
       "  'c',\n",
       "  'apply',\n",
       "  'spam'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divideset(words[2],0,'entry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create counts of possible results (the last column of each row is the result)\n",
    "def uniquecounts(rows):\n",
    "   results={}\n",
    "   for row in rows:\n",
    "      # The result is the last column\n",
    "      r=row[len(row)-1]\n",
    "      if r not in results: results[r]=0\n",
    "      results[r]+=1\n",
    "   return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spam': 745, 'ham': 4708}\n"
     ]
    }
   ],
   "source": [
    "print(uniquecounts(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Entropy is the sum of p(x)log(p(x)) across all \n",
    "# the different possible results\n",
    "def entropy(rows):\n",
    "   from math import log\n",
    "   log2=lambda x:log(x)/log(2)  \n",
    "   results=uniquecounts(rows)\n",
    "   # Now calculate the entropy\n",
    "   ent=0.0\n",
    "   for r in results.keys():\n",
    "      p=float(results[r])/len(rows)\n",
    "      ent=ent-p*log2(p)\n",
    "   return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "num_r = len(words[2])\n",
    "print(num_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5753234166539531"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class decisionnode:\n",
    "  def __init__(self,col=-1,value=None,results=None,tb=None,fb=None):\n",
    "    self.col=col\n",
    "    self.value=value\n",
    "    self.results=results\n",
    "    self.tb=tb\n",
    "    self.fb=fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildtree(rows,scoref=entropy): #rows is the set, either whole dataset or part of it in the recursive call, \n",
    "                                    #scoref is the method to measure heterogeneity. By default it's entropy.\n",
    "  if len(rows)==0: return decisionnode() #len(rows) is the number of units in a set\n",
    "  current_score=scoref(rows)\n",
    "\n",
    "  # Set up some variables to track the best criteria\n",
    "  best_gain=0.0\n",
    "  best_criteria=None\n",
    "  best_sets=None\n",
    "  \n",
    "  #column_count=len(rows[0])-1   #count the # of attributes/columns. \n",
    "                                #It's -1 because the last one is the target attribute and it does not count.\n",
    "        \n",
    "  #row_count=len(rows)      \n",
    "  for row in rows:\n",
    "    # Generate the list of all possible different values in the considered column\n",
    "    global column_values        #Added for debugging\n",
    "    column_values={} \n",
    "    col_count=len(row)-1\n",
    "    for col in range(0,col_count):\n",
    "       column_values[row[col]]=1   \n",
    "    # Now try dividing the rows up for each value in this column\n",
    "    for value in column_values.keys(): #the 'values' here are the keys of the dictionnary\n",
    "      (set1,set2)=divideset(rows,0,value) #define set1 and set2 as the 2 children set of a division\n",
    "      \n",
    "      # Information gain\n",
    "      p=float(len(set1))/len(rows) #p is the size of a child set relative to its parent\n",
    "      gain=current_score-p*scoref(set1)-(1-p)*scoref(set2) #cf. formula information gain\n",
    "      if gain>best_gain and len(set1)>0 and len(set2)>0: #set must not be empty\n",
    "        best_gain=gain\n",
    "        best_criteria=(col,value)\n",
    "        best_sets=(set1,set2)\n",
    "        \n",
    "  # Create the sub branches   \n",
    "  if best_gain>0:\n",
    "    trueBranch=buildtree(best_sets[0])\n",
    "    falseBranch=buildtree(best_sets[1])\n",
    "    return decisionnode(col=best_criteria[0],value=best_criteria[1],\n",
    "                        tb=trueBranch,fb=falseBranch)\n",
    "  else:\n",
    "    return decisionnode(results=uniquecounts(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-7b63506f35fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuildtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-ce648801fc2b>\u001b[0m in \u001b[0;36mbuildtree\u001b[1;34m(rows, scoref)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Now try dividing the rows up for each value in this column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumn_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#the 'values' here are the keys of the dictionnary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m       \u001b[1;33m(\u001b[0m\u001b[0mset1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mset2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdivideset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#define set1 and set2 as the 2 children set of a division\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m       \u001b[1;31m# Information gain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-d447e9f51479>\u001b[0m in \u001b[0;36mdivideset\u001b[1;34m(rows, column, value)\u001b[0m\n\u001b[0;32m     11\u001b[0m    \u001b[1;31m# Divide the rows into two sets and return them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m    \u001b[0mset1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrows\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msplit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m    \u001b[0mset2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrows\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msplit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mset1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mset2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-d447e9f51479>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m    \u001b[1;31m# Divide the rows into two sets and return them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m    \u001b[0mset1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrows\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msplit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m    \u001b[0mset2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrows\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msplit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mset1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mset2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-d447e9f51479>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      7\u001b[0m       \u001b[0msplit_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m       \u001b[0msplit_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m    \u001b[1;31m# Divide the rows into two sets and return them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tree=buildtree(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 17\n",
      "1 7\n",
      "2 22\n",
      "3 10\n",
      "4 8\n",
      "5 18\n",
      "6 9\n",
      "7 17\n",
      "8 17\n",
      "9 17\n",
      "10 13\n",
      "11 18\n",
      "12 19\n",
      "13 16\n",
      "14 3\n",
      "15 17\n",
      "16 4\n",
      "17 12\n",
      "18 10\n",
      "19 22\n"
     ]
    }
   ],
   "source": [
    "for n in range(0,20):\n",
    "    print(n,len(words[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tree.col)\n",
    "print(tree.value)\n",
    "print(tree.results)\n",
    "print(\"\")\n",
    "print(tree.tb.col)\n",
    "print(tree.tb.value)\n",
    "print(tree.tb.results)\n",
    "print(\"\")\n",
    "print(tree.tb.tb.col)\n",
    "print(tree.tb.tb.value)\n",
    "print(tree.tb.tb.results)\n",
    "print(\"\")\n",
    "print(tree.tb.fb.col)\n",
    "print(tree.tb.fb.value)\n",
    "print(tree.tb.fb.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printtree(tree,indent=''):\n",
    "   # Is this a leaf node?\n",
    "    if tree.results!=None:\n",
    "        print(str(tree.results))\n",
    "    else:\n",
    "        print(str(tree.col)+':'+str(tree.value)+'? ')\n",
    "        # Print the branches\n",
    "        print(indent+'T->', end=\" \")\n",
    "        printtree(tree.tb,indent+'  ')\n",
    "        print(indent+'F->', end=\" \")\n",
    "        printtree(tree.fb,indent+'  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "printtree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getwidth(tree):\n",
    "  if tree.tb==None and tree.fb==None: return 1\n",
    "  return getwidth(tree.tb)+getwidth(tree.fb)\n",
    "\n",
    "def getdepth(tree):\n",
    "  if tree.tb==None and tree.fb==None: return 0\n",
    "  return max(getdepth(tree.tb),getdepth(tree.fb))+1\n",
    "\n",
    "\n",
    "from PIL import Image,ImageDraw\n",
    "\n",
    "def drawtree(tree,jpeg='tree.jpg'):\n",
    "  w=getwidth(tree)*100\n",
    "  h=getdepth(tree)*100+120\n",
    "\n",
    "  img=Image.new('RGB',(w,h),(255,255,255))\n",
    "  draw=ImageDraw.Draw(img)\n",
    "\n",
    "  drawnode(draw,tree,w/2,20)\n",
    "  img.save(jpeg,'JPEG')\n",
    "  \n",
    "def drawnode(draw,tree,x,y):\n",
    "  if tree.results==None:\n",
    "    # Get the width of each branch\n",
    "    w1=getwidth(tree.fb)*100\n",
    "    w2=getwidth(tree.tb)*100\n",
    "\n",
    "    # Determine the total space required by this node\n",
    "    left=x-(w1+w2)/2\n",
    "    right=x+(w1+w2)/2\n",
    "\n",
    "    # Draw the condition string\n",
    "    draw.text((x-20,y-10),str(tree.col)+':'+str(tree.value),(0,0,0))\n",
    "\n",
    "    # Draw links to the branches\n",
    "    draw.line((x,y,left+w1/2,y+100),fill=(255,0,0))\n",
    "    draw.line((x,y,right-w2/2,y+100),fill=(255,0,0))\n",
    "    \n",
    "    # Draw the branch nodes\n",
    "    drawnode(draw,tree.fb,left+w1/2,y+100)\n",
    "    drawnode(draw,tree.tb,right-w2/2,y+100)\n",
    "  else:\n",
    "    txt=' \\n'.join(['%s:%d'%v for v in tree.results.items()])\n",
    "    draw.text((x-20,y),txt,(0,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drawtree(tree,jpeg='treeview.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
